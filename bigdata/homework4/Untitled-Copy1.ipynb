{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "apparent-genius",
   "metadata": {},
   "source": [
    "1 Preprocessing (20 pts)\n",
    "\n",
    "Please leave your codes and results in the following sub-questions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "invisible-guyana",
   "metadata": {},
   "source": [
    "(a) First we will load the data using spark data source API. Write codes to load and print\n",
    "out its schema using printSchema()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "intellectual-updating",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from pyspark.mllib.stat import Statistics\n",
    "from pyspark import SparkContext\n",
    "sc = SparkContext()\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "\n",
    "sqlContext = SQLContext(sc)\n",
    "spark = SparkSession.builder.appName('Spark Session 1').getOrCreate()\n",
    "df = spark.read.csv('test.csv', header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "previous-interaction",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- PassengerId: string (nullable = true)\n",
      " |-- Pclass: string (nullable = true)\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Sex: string (nullable = true)\n",
      " |-- Age: string (nullable = true)\n",
      " |-- SibSp: string (nullable = true)\n",
      " |-- Parch: string (nullable = true)\n",
      " |-- Ticket: string (nullable = true)\n",
      " |-- Fare: string (nullable = true)\n",
      " |-- Cabin: string (nullable = true)\n",
      " |-- Embarked: string (nullable = true)\n",
      "\n",
      "root\n",
      " |-- PassengerId: string (nullable = true)\n",
      " |-- Survived: string (nullable = true)\n",
      " |-- Pclass: string (nullable = true)\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Sex: string (nullable = true)\n",
      " |-- Age: string (nullable = true)\n",
      " |-- SibSp: string (nullable = true)\n",
      " |-- Parch: string (nullable = true)\n",
      " |-- Ticket: string (nullable = true)\n",
      " |-- Fare: string (nullable = true)\n",
      " |-- Cabin: string (nullable = true)\n",
      " |-- Embarked: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_test = spark.read.csv('test.csv', header=True)\n",
    "df_train = spark.read.csv('train.csv', header=True)\n",
    "df_test.printSchema()\n",
    "df_train.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "fatty-responsibility",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+------+--------------------+------+---------+-----+-----+-------+--------+\n",
      "|PassengerId|Survived|Pclass|                Name|   Sex|      Age|SibSp|Parch|   Fare|Embarked|\n",
      "+-----------+--------+------+--------------------+------+---------+-----+-----+-------+--------+\n",
      "|          1|     0.0|     3|Braund, Mr. Owen ...|  male|     22.0|    1|    0|   7.25|       S|\n",
      "|          2|     1.0|     1|Cumings, Mrs. Joh...|female|     38.0|    1|    0|71.2833|       C|\n",
      "|          3|     1.0|     3|Heikkinen, Miss. ...|female|     26.0|    0|    0|  7.925|       S|\n",
      "|          4|     1.0|     1|Futrelle, Mrs. Ja...|female|     35.0|    1|    0|   53.1|       S|\n",
      "|          5|     0.0|     3|Allen, Mr. Willia...|  male|     35.0|    0|    0|   8.05|       S|\n",
      "|          6|     0.0|     3|    Moran, Mr. James|  male|29.699118|    0|    0| 8.4583|       Q|\n",
      "|          7|     0.0|     1|McCarthy, Mr. Tim...|  male|     54.0|    0|    0|51.8625|       S|\n",
      "|          8|     0.0|     3|Palsson, Master. ...|  male|      2.0|    3|    1| 21.075|       S|\n",
      "|          9|     1.0|     3|Johnson, Mrs. Osc...|female|     27.0|    0|    2|11.1333|       S|\n",
      "|         10|     1.0|     2|Nasser, Mrs. Nich...|female|     14.0|    1|    0|30.0708|       C|\n",
      "|         11|     1.0|     3|Sandstrom, Miss. ...|female|      4.0|    1|    1|   16.7|       S|\n",
      "|         12|     1.0|     1|Bonnell, Miss. El...|female|     58.0|    0|    0|  26.55|       S|\n",
      "|         13|     0.0|     3|Saundercock, Mr. ...|  male|     20.0|    0|    0|   8.05|       S|\n",
      "|         14|     0.0|     3|Andersson, Mr. An...|  male|     39.0|    1|    5| 31.275|       S|\n",
      "|         15|     0.0|     3|Vestrom, Miss. Hu...|female|     14.0|    0|    0| 7.8542|       S|\n",
      "|         16|     1.0|     2|Hewlett, Mrs. (Ma...|female|     55.0|    0|    0|     16|       S|\n",
      "|         17|     0.0|     3|Rice, Master. Eugene|  male|      2.0|    4|    1| 29.125|       Q|\n",
      "|         18|     1.0|     2|Williams, Mr. Cha...|  male|29.699118|    0|    0|     13|       S|\n",
      "|         19|     0.0|     3|Vander Planke, Mr...|female|     31.0|    1|    0|     18|       S|\n",
      "|         20|     1.0|     3|Masselmani, Mrs. ...|female|29.699118|    0|    0|  7.225|       C|\n",
      "+-----------+--------+------+--------------------+------+---------+-----+-----+-------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_train.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "jewish-calendar",
   "metadata": {},
   "source": [
    "(b) Find columns that include Missing values. After that, fill the mean value for all the\n",
    "missing values. Please drop Ticket and Cabin columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "egyptian-court",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Pclass', 'Age', 'Embarked', 'Fare', 'Parch', 'Sex', 'SibSp', 'Name', 'PassengerId'}\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "columns = set(df_test.columns) - set(['Ticket', 'Cabin'])\n",
    "print(columns)\n",
    "df_test = df_test.drop('Ticket')\n",
    "df_test = df_test.drop('Cabin')\n",
    "\n",
    "df_train = df_train.drop('Ticket')\n",
    "df_train = df_train.drop('Cabin')\n",
    "#df_train.where(null).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "surface-disclosure",
   "metadata": {},
   "outputs": [],
   "source": [
    "def null_value_count(df):\n",
    "    null_columns_counts = []\n",
    "    numRows = df.count()\n",
    "    for k in df.columns:\n",
    "        nullRows = df.where(col(k).isNull()).count()\n",
    "        if(nullRows > 0):\n",
    "            temp = k,nullRows\n",
    "            null_columns_counts.append(temp)\n",
    "    return(null_columns_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "downtown-germany",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Age', 177), ('Embarked', 2)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "null_columns_count_list = null_value_count(df_train)\n",
    "null_columns_count_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "laden-neighborhood",
   "metadata": {},
   "outputs": [],
   "source": [
    "#test\n",
    "mean_Age_train = df_train.select(mean('Age')).collect()[0][0]\n",
    "#mean_Embarked_train = df_train.select(mean('Embarked')).collect()[0][0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "gothic-folks",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "df_train = df_train.na.fill({\"Age\" : mean_Age_train})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "corrected-connectivity",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Embarked='Q', count=77),\n",
       " Row(Embarked=None, count=2),\n",
       " Row(Embarked='C', count=168),\n",
       " Row(Embarked='S', count=644)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.groupBy('Embarked').count().collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "realistic-collar",
   "metadata": {},
   "source": [
    "S is most frequent value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "floral-speaker",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train.na.fill({'Embarked': '0'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stuck-transcription",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train.withColumn('prediction', df_train['prediction'].cast(\"integer\").alias('prediction'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "civilian-parker",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hist = df_train.select('Age').rdd.flatMap(lambda x:x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "miniature-omaha",
   "metadata": {},
   "source": [
    "(c) Visualize a histogram for an Age column.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "horizontal-stuart",
   "metadata": {},
   "outputs": [],
   "source": [
    "toFloat = ['Age', 'PassengerId', 'Pclass', 'SibSp', 'Parch', 'Fare']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "fuzzy-extra",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- PassengerId: float (nullable = true)\n",
      " |-- Survived: float (nullable = true)\n",
      " |-- Pclass: float (nullable = true)\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Sex: string (nullable = true)\n",
      " |-- Age: float (nullable = true)\n",
      " |-- SibSp: float (nullable = true)\n",
      " |-- Parch: float (nullable = true)\n",
      " |-- Fare: float (nullable = true)\n",
      " |-- Embarked: string (nullable = false)\n",
      "\n",
      "root\n",
      " |-- PassengerId: float (nullable = true)\n",
      " |-- Survived: float (nullable = true)\n",
      " |-- Pclass: float (nullable = true)\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Sex: string (nullable = true)\n",
      " |-- Age: float (nullable = true)\n",
      " |-- SibSp: float (nullable = true)\n",
      " |-- Parch: float (nullable = true)\n",
      " |-- Fare: float (nullable = true)\n",
      " |-- Embarked: string (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_train.printSchema()\n",
    "\n",
    "for col in toFloat:\n",
    "    df_train = df_train.withColumn(col, df_train[col].cast(\"float\").alias(col))\n",
    "\n",
    "\n",
    "df_train.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "helpful-invite",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 54.,  46., 177., 346., 118.,  70.,  45.,  24.,   9.,   2.]),\n",
       " array([ 0.41999999,  8.37799999, 16.33599999, 24.29399999, 32.25199999,\n",
       "        40.20999999, 48.16799999, 56.126     , 64.084     , 72.042     ,\n",
       "        80.        ]),\n",
       " <BarContainer object of 10 artists>)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark_dist_explore import hist\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.set_xlabel('age')\n",
    "ax.set_ylabel('count')\n",
    "hist(ax, df_train.select('Age'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "rapid-leonard",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ax = df_train[\"Age\"].hist(bins=15, density=True, stacked=True, color=\\'teal\\', alpha=0.6)\\ndf_train[\"Age\"].plot(kind=\\'density\\', color=\\'teal\\')\\nax.set(xlabel=\\'Age\\')\\nplt.xlim(-10,85)\\nplt.show()\\npandas로 해보자'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"ax = df_train[\"Age\"].hist(bins=15, density=True, stacked=True, color='teal', alpha=0.6)\n",
    "df_train[\"Age\"].plot(kind='density', color='teal')\n",
    "ax.set(xlabel='Age')\n",
    "plt.xlim(-10,85)\n",
    "plt.show()\n",
    "pandas로 해보자\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "graduate-extraction",
   "metadata": {},
   "source": [
    "(d) Many columns are categorical variables. So we use one-hot encoding using spark ML\n",
    "pipeline API. In this example, we are using StringIndexer and OneHotEncoder to do\n",
    "that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "extended-press",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'OneHotEncoderEstimator' from 'pyspark.ml.feature' (/home/poco/.local/lib/python3.8/site-packages/pyspark/ml/feature.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-68-f3bed1d38791>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mOneHotEncoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mStringIndexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mVectorIndexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mVectorAssembler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOneHotEncoderEstimator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mml\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPipeline\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'OneHotEncoderEstimator' from 'pyspark.ml.feature' (/home/poco/.local/lib/python3.8/site-packages/pyspark/ml/feature.py)"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer, VectorIndexer, VectorAssembler, OneHotEncoderEstimator\n",
    "from pyspark.ml import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "directed-haiti",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[PassengerId: string, Survived: float, Pclass: string, Name: string, Sex: string, Age: float, SibSp: string, Parch: string, Fare: string, Embarked: string]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sur = df_train.select('Survived').collect()\n",
    "#df_tmp = df_train.drop('Survived')\n",
    "\"\"\"col_list = df_train.columns\n",
    "col_list\"\"\"\n",
    "col_list = df_train.columns\n",
    "col_list\n",
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "czech-inspector",
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_cols = ['PassengerId','Survived', 'Pclass',\n",
    "                'Age', 'SibSp','Parch', 'Fare'] \n",
    "numeric_features = ['PassengerId','Pclass','Age', 'SibSp','Parch','Fare'] \n",
    "string_features = ['Name', 'Sex'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "persistent-immune",
   "metadata": {},
   "outputs": [],
   "source": [
    "_stages = []\n",
    "string_indexer =  [StringIndexer(inputCol = column , \\\n",
    "                                 outputCol = column + '_StringIndexer', \n",
    "                                 handleInvalid = \"skip\") for column in string_features]\n",
    "\n",
    "one_hot_encoder = [OneHotEncoder(\n",
    "    inputCols = [column + '_StringIndexer' for column in string_features ], \\\n",
    "    outputCols =  [column + '_OneHotEncoderEstimator' for column in string_features ])]\n",
    "\n",
    "vect_indexer = [VectorIndexer(\n",
    "    inputCol = column + '_OneHotEncoderEstimator',\n",
    "    outputCol = column + '_VectorIndexer', \n",
    "    maxCategories=10) for column in string_features]\n",
    "\n",
    "\n",
    "assemblerInput =  [f  for f in numeric_features]  \n",
    "assemblerInput += [f + \"_VectorIndexer\" for f in string_features]\n",
    "vector_assembler = VectorAssembler(inputCols = assemblerInput, \\\n",
    "                                   outputCol = 'VectorAssembler_features')\n",
    "\n",
    "_stages += string_indexer\n",
    "_stages += one_hot_encoder\n",
    "_stages += vect_indexer\n",
    "_stages += [vector_assembler]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "thousand-trinidad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[StringIndexer_675db04378cc, OneHotEncoder_8ac640e0b295]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_stages = []\n",
    "#indexers =[StringIndexer(inputCol=column, outputCol=column+'_index') for column in col_list]\n",
    "indexers = [StringIndexer(inputCols = [col for col in col_list], outputCols = [col+'_index' for col in col_list])]\n",
    "\n",
    "encoders = [OneHotEncoder(inputCols = [col+'_index' for col in col_list], outputCols = [col+'_vec' for col in col_list])]\n",
    "#pipeline = Pipeline(stages=indexers)\n",
    "#df_pip = pipeline.fit(df_tmp).transform(df_tmp)\n",
    "\n",
    "_stages += indexers\n",
    "_stages += encoders\n",
    "_stages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "advisory-binary",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[StringIndexer_675db04378cc,\n",
       " OneHotEncoder_8ac640e0b295,\n",
       " VectorAssembler_900b2c869664]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "asm_input = [col + '_index' for col in col_list]\n",
    "vector_assembler = VectorAssembler(inputCols=asm_input, outputCol='features')\n",
    "_stages += [vector_assembler]\n",
    "_stages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "demonstrated-sheet",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(stages = _stages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "interim-appeal",
   "metadata": {},
   "source": [
    "(e) One we prepared our data, we split the data into two sets: training (80 %) and testing\n",
    "(20 %) datasets. We use Spark’s randomSplit method to get them. Please leave the\n",
    "code.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "boolean-vulnerability",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tmp = df_train\n",
    "#train, test = df_train.randomSplit([0.8, 0.2])\n",
    "#df_tmp = df_tmp.drop('Survived')\n",
    "df_tmp\n",
    "df_train = df_train.withColumn('Survived', df_train['Survived'].cast(\"float\").alias('Survived'))\n",
    "train, test = df_train.randomSplit([0.8, 0.2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "foreign-boxing",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "701\n",
      "190\n",
      "DataFrame[PassengerId: float, Survived: float, Pclass: float, Name: string, Sex: string, Age: float, SibSp: float, Parch: float, Fare: float, Embarked: string]\n",
      "DataFrame[PassengerId: float, Survived: float, Pclass: float, Name: string, Sex: string, Age: float, SibSp: float, Parch: float, Fare: float, Embarked: string]\n"
     ]
    }
   ],
   "source": [
    "print(train.count())\n",
    "print(test.count())\n",
    "print(test)\n",
    "print(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fitted-deputy",
   "metadata": {},
   "source": [
    "2 Classification using Logistic Regression (20 pts)\n",
    "\n",
    "In this questions below, you will train and test a logistic regression model. Please leave\n",
    "your codes for each part in your report.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "surgical-integer",
   "metadata": {},
   "source": [
    "(a) Create stages for all features, which are processed above, and make a pipeline with\n",
    "logistic regression using default parameters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "expensive-champion",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.feature import VectorAssembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "talented-chain",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[StringIndexer_608aafb8208c,\n",
       " StringIndexer_e77a41194238,\n",
       " OneHotEncoder_16ead623949c,\n",
       " VectorIndexer_2334a14fb556,\n",
       " VectorIndexer_2441566499ca,\n",
       " VectorAssembler_e3d4c7a9a155,\n",
       " LogisticRegression_27aea7426635]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr = LogisticRegression(labelCol='Survived')\n",
    "_stages += [lr]\n",
    "_stages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "painted-julian",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(stages = _stages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "golden-virus",
   "metadata": {},
   "source": [
    "(b) Fit the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "classified-contrary",
   "metadata": {},
   "outputs": [
    {
     "ename": "IllegalArgumentException",
     "evalue": "features does not exist. Available: PassengerId, Survived, Pclass, Name, Sex, Age, SibSp, Parch, Fare, Embarked, Name_StringIndexer, Sex_StringIndexer, Name_OneHotEncoderEstimator, Sex_OneHotEncoderEstimator, Name_VectorIndexer, Sex_VectorIndexer, VectorAssembler_features",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIllegalArgumentException\u001b[0m                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-64-f38c0a657443>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpipeline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtrain_predict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'*'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtrain_predict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pyspark/ml/base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    159\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 161\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    162\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m             raise ValueError(\"Params must be either a param map or a list/tuple of param maps, \"\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pyspark/ml/pipeline.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    112\u001b[0m                     \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# must be an Estimator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m                     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m                     \u001b[0mtransformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mindexOfLastEstimator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pyspark/ml/base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    159\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 161\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    162\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m             raise ValueError(\"Params must be either a param map or a list/tuple of param maps, \"\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    333\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 335\u001b[0;31m         \u001b[0mjava_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    336\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjava_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    337\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_copyValues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_fit_java\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    330\u001b[0m         \"\"\"\n\u001b[1;32m    331\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transfer_params_to_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 332\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_java_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    333\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1304\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1305\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    115\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIllegalArgumentException\u001b[0m: features does not exist. Available: PassengerId, Survived, Pclass, Name, Sex, Age, SibSp, Parch, Fare, Embarked, Name_StringIndexer, Sex_StringIndexer, Name_OneHotEncoderEstimator, Sex_OneHotEncoderEstimator, Name_VectorIndexer, Sex_VectorIndexer, VectorAssembler_features"
     ]
    }
   ],
   "source": [
    "model = pipeline.fit(train)\n",
    "train_predict = model.transform(train).select('*')\n",
    "train_predict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "seasonal-cross",
   "metadata": {},
   "source": [
    "(c) Once model is trained, we need to know how it’s performing. So we use precision\n",
    "score as our evaluation metric. Report the result. You can refer to RDD-Evaluation\n",
    "or Precision with Micro option (Scikit-learn)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "referenced-burns",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "test_predict = model.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "increased-rogers",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = test_predict.select('PassengerId', 'probability', 'prediction')\n",
    "#prediction.withColumn('Survived', prediction['prediction'].cast('integer')).drop('prediction')\n",
    "#prediction = prediction.withColumn('prediction', prediction['prediction'].cast(\"integer\").alias('prediction'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "atmospheric-fields",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o3746.collectToPython.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 337.0 failed 1 times, most recent failure: Lost task 0.0 in stage 337.0 (TID 541) (172.18.242.120 executor driver): org.apache.spark.SparkException: Failed to execute user defined function(StringIndexerModel$$Lambda$3218/1156877334: (string) => double)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:345)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:898)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:898)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.SparkException: Unseen label: 104. To handle unseen labels, set Param handleInvalid to keep.\n\tat org.apache.spark.ml.feature.StringIndexerModel.$anonfun$getIndexer$1(StringIndexer.scala:406)\n\tat org.apache.spark.ml.feature.StringIndexerModel.$anonfun$getIndexer$1$adapted(StringIndexer.scala:391)\n\t... 17 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2253)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2202)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2201)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2201)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1078)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1078)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1078)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2440)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2382)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2371)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:868)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2202)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2223)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2242)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2267)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1030)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:414)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1029)\n\tat org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:390)\n\tat org.apache.spark.sql.Dataset.$anonfun$collectToPython$1(Dataset.scala:3519)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3687)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3685)\n\tat org.apache.spark.sql.Dataset.collectToPython(Dataset.scala:3516)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.SparkException: Failed to execute user defined function(StringIndexerModel$$Lambda$3218/1156877334: (string) => double)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:345)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:898)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:898)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\nCaused by: org.apache.spark.SparkException: Unseen label: 104. To handle unseen labels, set Param handleInvalid to keep.\n\tat org.apache.spark.ml.feature.StringIndexerModel.$anonfun$getIndexer$1(StringIndexer.scala:406)\n\tat org.apache.spark.ml.feature.StringIndexerModel.$anonfun$getIndexer$1$adapted(StringIndexer.scala:391)\n\t... 17 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-99-9c1e938427b0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mprediction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mrid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mcollect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    675\u001b[0m         \"\"\"\n\u001b[1;32m    676\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mSCCallSiteSync\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcss\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 677\u001b[0;31m             \u001b[0msock_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollectToPython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    678\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msock_info\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBatchedSerializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPickleSerializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    679\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1304\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1305\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOUTPUT_CONVERTER\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mREFERENCE_TYPE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m                 raise Py4JJavaError(\n\u001b[0m\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m                     format(target_id, \".\", name), value)\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o3746.collectToPython.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 337.0 failed 1 times, most recent failure: Lost task 0.0 in stage 337.0 (TID 541) (172.18.242.120 executor driver): org.apache.spark.SparkException: Failed to execute user defined function(StringIndexerModel$$Lambda$3218/1156877334: (string) => double)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:345)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:898)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:898)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.SparkException: Unseen label: 104. To handle unseen labels, set Param handleInvalid to keep.\n\tat org.apache.spark.ml.feature.StringIndexerModel.$anonfun$getIndexer$1(StringIndexer.scala:406)\n\tat org.apache.spark.ml.feature.StringIndexerModel.$anonfun$getIndexer$1$adapted(StringIndexer.scala:391)\n\t... 17 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2253)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2202)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2201)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2201)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1078)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1078)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1078)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2440)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2382)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2371)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:868)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2202)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2223)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2242)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2267)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1030)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:414)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1029)\n\tat org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:390)\n\tat org.apache.spark.sql.Dataset.$anonfun$collectToPython$1(Dataset.scala:3519)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3687)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3685)\n\tat org.apache.spark.sql.Dataset.collectToPython(Dataset.scala:3516)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.SparkException: Failed to execute user defined function(StringIndexerModel$$Lambda$3218/1156877334: (string) => double)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:345)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:898)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:898)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\nCaused by: org.apache.spark.SparkException: Unseen label: 104. To handle unseen labels, set Param handleInvalid to keep.\n\tat org.apache.spark.ml.feature.StringIndexerModel.$anonfun$getIndexer$1(StringIndexer.scala:406)\n\tat org.apache.spark.ml.feature.StringIndexerModel.$anonfun$getIndexer$1$adapted(StringIndexer.scala:391)\n\t... 17 more\n"
     ]
    }
   ],
   "source": [
    "for row in prediction.collect():\n",
    "    rid, prob, prec = row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "lovely-craft",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluator = MulticlassClassificationEvaluator(labelCol='prediction', metricName='accuracy')\n",
    "evaluator.evaluate(train_predict)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
