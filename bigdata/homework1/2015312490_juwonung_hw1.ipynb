{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "# Homework 1 (Due: Mar. 17, 2021 (11:59 PM))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Name: Your name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Student ID: Your ID\n",
    "Late submission Days used here: 0 (if there is, please modify here)\n",
    "\n",
    "[Please SUBMIT (1) YOUR IPYNB AND (2) PDF (please use FILE/DOWNLOAD AS/PDF or PRINT PREVIEW/PRINT AS PDF with \"printed output\") TO iCampus]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "For this homework you cannot use the python library scikit-learn (sklearn). \n",
    "You can use the python package BeautifulSoup to parse web pages.\n",
    "\n",
    "In this assignment you will retrieve and parse webpages. The text file \"urls.txt\" contains a list of urls for the webpages to be parsed. Each line in the text file corresponds to a url. Use BeautifulSoup to fetch each webpage.\n",
    "\n",
    "Note: For all questions, the words should be converted to lower case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "# Q1: Part 1 (5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "Parse the first webpage document to retrieve the text enclosed in paragraph tags, find the words that end in \"ing\", and count how many times each word appears. \n",
    "\n",
    "Sort these words in decreasing order of frequency, and write the words (along with their corresponding frequencies) in an output file named \"Q1_Part1.txt\". The most frequent word should appear at the top and the least frequent word at the end, and the format of the output file should be:\n",
    "word TAB frequency\n",
    "\n",
    "Example:\n",
    "\n",
    "sorting\t10\n",
    "training\t8\n",
    "broadening\t6\n",
    "extracting\t3\n",
    "evergrowing\t2\n",
    "coming\t1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR CODE HERE\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "url = \"https://en.wikipedia.org/wiki/Data_science\"\n",
    "    \n",
    "response = requests.get(url)\n",
    "\n",
    "if response.status_code==200:\n",
    "    html = response.text\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    paragraphs = soup.find_all('p')\n",
    "    #text_list = text.split()\n",
    "    \n",
    "   \n",
    "    \n",
    "else:\n",
    "    print(response.status_code)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    str  count\n",
      "0  none      0\n"
     ]
    }
   ],
   "source": [
    "d = {'str':['none'], 'count':[0]}\n",
    "df = pd.DataFrame(data=d)\n",
    "print(df[df['str']=='none'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in paragraphs:\n",
    "    p_text = p.get_text()\n",
    "    pattern = re.compile(r\"[a-zA-Z]+ing\",re.IGNORECASE)\n",
    "    #pattern = re.compile(\".*ing$\", )\n",
    "    datas = re.findall(pattern, p_text)\n",
    "    #print(datas)\n",
    "    for data in datas:\n",
    "        data = data.lower()\n",
    "        if df['str'][0]=='none':\n",
    "            df.iloc[0,0] = data\n",
    "            df.iloc[0, 1] += 1\n",
    "        \n",
    "        else:\n",
    "            if not df[df['str']==data].empty:\n",
    "                idx = df[df['str']==data].index\n",
    "                df.iloc[idx,1]+=1\n",
    "                \n",
    "            else:\n",
    "                df = df.append({'str':data, 'count':1}, ignore_index=True)\n",
    "        \n",
    "\n",
    "df_sorted = df.sort_values(by=['count', 'str'], axis=0, ascending=[False, True])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(34, 2)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sorted\n",
    "df_sorted.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learning 5\n",
      "mining 3\n",
      "beijing 2\n",
      "computing 2\n",
      "finding 2\n",
      "accounting 1\n",
      "analyzing 1\n",
      "applying 1\n",
      "becoming 1\n",
      "being 1\n",
      "breaking 1\n",
      "changing 1\n",
      "combining 1\n",
      "creating 1\n",
      "describing 1\n",
      "developing 1\n",
      "disting 1\n",
      "drawing 1\n",
      "during 1\n",
      "emerging 1\n",
      "enabling 1\n",
      "everything 1\n",
      "extracting 1\n",
      "formulating 1\n",
      "growing 1\n",
      "including 1\n",
      "increasing 1\n",
      "managing 1\n",
      "misleading 1\n",
      "preparing 1\n",
      "presenting 1\n",
      "reflecting 1\n",
      "training 1\n",
      "turing 1\n"
     ]
    }
   ],
   "source": [
    "#store dataframe\n",
    "file = open('./Q1_Part1.txt', 'w')\n",
    "\n",
    "for row_index, value in df_sorted.iterrows():\n",
    "    #print(value['str'])\n",
    "    file.write(value['str']+ \" \"+ str(value['count']))\n",
    "    print(value['str']+ \" \"+ str(value['count']))\n",
    "    file.write('\\n')\n",
    "file.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "# Q1: Part 2 (5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "Stop words are natural language words which have very little meaning, such as \"and\", \"the\", \"a\", \"an\", and similar words.\n",
    "\n",
    "Repeat Part 1, but before counting, remove the stop words given in the file \"stop_words.txt\". The ouput for Part 2 should have the same format as Part 1, and should be written to an output file named \"Q1_Part2.txt\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR CODE HERE\n",
    "stop_words = pd.read_csv('./stop_words.txt', engine='python', encoding='UTF-8', sep='\\n', header=None)\n",
    "\n",
    "text = \"\"\n",
    "#stop_word 제거\n",
    "\n",
    "#전체 텍스트 하나 만들기\n",
    "for p in paragraphs:\n",
    "        p_text = p.get_text()\n",
    "        p_text = p_text.lower()\n",
    "        if text==\"\":\n",
    "            text = p_text\n",
    "        else: \n",
    "            text = text + \" \" + p_text\n",
    "\n",
    "text = re.sub(r'[\\W\\d]',' ',text)\n",
    "text = text.split(' ')\n",
    "text = list(filter(None, text))\n",
    "            \n",
    "\n",
    "for index, word in stop_words.iterrows():\n",
    "    while word.iloc[0] in text:\n",
    "        text.remove(word.iloc[0])\n",
    "    \n",
    "para = \"\"\n",
    "for word in text:\n",
    "    if para == \"\":\n",
    "        para = word\n",
    "    else :\n",
    "        para = para + \" \" + word\n",
    "\n",
    "#print(para)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = {'str':['none'], 'count':[0]}\n",
    "df1_1 = pd.DataFrame(data=d)\n",
    "\n",
    "pattern = re.compile(r\"[a-zA-Z]+ing\",re.IGNORECASE)\n",
    "#pattern = re.compile(\".*ing$\", )\n",
    "datas = re.findall(pattern, para)\n",
    "\n",
    "\n",
    "for data in datas:\n",
    "    data = data.lower()\n",
    "    if df1_1['str'][0]=='none':\n",
    "        df1_1.iloc[0,0] = data\n",
    "        df1_1.iloc[0, 1] += 1\n",
    "    \n",
    "    else:\n",
    "        if not df1_1[df1_1['str']==data].empty:\n",
    "            idx = df1_1[df1_1['str']==data].index\n",
    "            df1_1.iloc[idx,1]+=1\n",
    "                \n",
    "        else:\n",
    "            df1_1 = df1_1.append({'str':data, 'count':1}, ignore_index=True)\n",
    "        \n",
    "\n",
    "df1_1_sorted = df1_1.sort_values(by=['count', 'str'], axis=0, ascending=[False, True])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30, 2)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1_1_sorted.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learning 5\n",
      "mining 3\n",
      "beijing 2\n",
      "computing 2\n",
      "finding 2\n",
      "accounting 1\n",
      "analyzing 1\n",
      "applying 1\n",
      "breaking 1\n",
      "changing 1\n",
      "combining 1\n",
      "creating 1\n",
      "describing 1\n",
      "developing 1\n",
      "disting 1\n",
      "drawing 1\n",
      "emerging 1\n",
      "enabling 1\n",
      "extracting 1\n",
      "formulating 1\n",
      "growing 1\n",
      "including 1\n",
      "increasing 1\n",
      "managing 1\n",
      "misleading 1\n",
      "preparing 1\n",
      "presenting 1\n",
      "reflecting 1\n",
      "training 1\n",
      "turing 1\n"
     ]
    }
   ],
   "source": [
    "#store dataframe\n",
    "file1_1 = open('./Q1_Part2.txt', 'w')\n",
    "\n",
    "for row_index, value in df1_1_sorted.iterrows():\n",
    "    #print(value['str'])\n",
    "    file1_1.write(value['str']+ \" \"+ str(value['count']))\n",
    "    print(value['str']+ \" \"+ str(value['count']))\n",
    "    file1_1.write('\\t')\n",
    "file1_1.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q2 (10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "Again, parse the first webpage document, but this time find and count all outgoing links to other webpages, and write the output to a file named \"Q2.txt\", with each url on a new line.\n",
    "\n",
    "The format of the output file should: number of outgoing urls in the first line, followed by each url on a new line. \n",
    "For example, \n",
    "4\n",
    "https://eng.skku.edu/eng/edu/education.do\n",
    "https://eng.skku.edu/eng/Research/industry/researchStory.do\n",
    "https://eng.skku.edu/eng/Univ-Industry/Research-Business-Found/FactsandFigures.do\n",
    "https://eng.skku.edu/eng/CampusLife/support/employment.do\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "372\n",
      "https://en.wikipedia.org/wiki/Data_science\n",
      "https://en.wikipedia.org/wiki/Recurrent_neural_network\n",
      "https://lv.wikipedia.org/wiki/Datu_m%C4%81c%C4%ABba\n",
      "http://creativecommons.org/licenses/by-sa/3.0/\n",
      "https://en.wikipedia.org/wiki/AnyChart\n",
      "https://en.wikipedia.org/wiki/Data_library\n",
      "https://en.wikipedia.org/wiki/State%E2%80%93action%E2%80%93reward%E2%80%93state%E2%80%93action\n",
      "https://en.wikipedia.org/wiki/Special:MyContributions\n",
      "https://en.wikipedia.org/w/index.php?title=Template:Machine_learning_bar&action=edit\n",
      "https://doi.org/10.1007%2F978-4-431-65950-1_3\n",
      "https://en.wikipedia.org/wiki/Data_loading\n",
      "https://en.wikipedia.org/wiki/Help:Introduction\n",
      "https://en.wikipedia.org/wiki/Logistic_regression\n",
      "https://en.wikipedia.org/wiki/BIRCH\n",
      "https://en.wikipedia.org/wiki/Information_explosion\n",
      "https://ko.wikipedia.org/wiki/%EB%8D%B0%EC%9D%B4%ED%84%B0_%EC%82%AC%EC%9D%B4%EC%96%B8%EC%8A%A4\n",
      "https://en.wikipedia.org/wiki/List_of_datasets_for_machine-learning_research\n",
      "https://en.wikipedia.org/wiki/Autoencoder\n",
      "https://en.wikipedia.org/wiki/Data_retention\n",
      "https://en.wikipedia.org/wiki/Data_archaeology\n",
      "https://en.wikipedia.org/wiki/Wikipedia:File_Upload_Wizard\n",
      "https://doi.org/10.1145%2F3076253\n",
      "https://commons.wikimedia.org/wiki/Category:Data_science\n",
      "https://en.wikipedia.org/wiki/Julia_(programming_language)\n",
      "https://en.wikipedia.org/wiki/C.F._Jeff_Wu\n",
      "https://en.wikipedia.org/wiki/Information_privacy\n",
      "https://en.wikipedia.org/wiki/K-means_clustering\n",
      "https://en.wikipedia.org/wiki/International_Conference_on_Machine_Learning\n",
      "https://en.wikipedia.org/wiki/Wikipedia:General_disclaimer\n",
      "https://en.wikipedia.org/wiki/ISSN_(identifier)\n",
      "https://en.wikipedia.org/wiki/Data_extraction\n",
      "https://en.wikipedia.org/w/index.php?title=Special:DownloadAsPdf&page=Data_science&action=show-download-screen\n",
      "https://en.wikipedia.org/wiki/Databricks\n",
      "https://en.wikipedia.org/wiki/Pytorch\n",
      "https://books.google.com/books?id=oGs_AQAAIAAJ\n",
      "https://en.wikipedia.org/wiki/Category:Articles_with_short_description\n",
      "https://en.wikipedia.org/wiki/Expectation%E2%80%93maximization_algorithm\n",
      "https://en.wikipedia.org/wiki/Special:MyTalk\n",
      "https://ca.wikipedia.org/wiki/Ci%C3%A8ncia_de_les_dades\n",
      "https://it.wikipedia.org/wiki/Scienza_dei_dati\n",
      "https://simple.wikipedia.org/wiki/Data_science\n",
      "https://web.archive.org/web/20141109113411/http://cacm.acm.org/magazines/2013/12/169933-data-science-and-prediction/fulltext\n",
      "https://et.wikipedia.org/wiki/Andmeteadus\n",
      "https://en.wikipedia.org/wiki/Business_intelligence\n",
      "https://en.wikipedia.org/w/index.php?title=Data_science&printable=yes\n",
      "https://tr.wikipedia.org/wiki/Veri_bilimi\n",
      "https://en.wikipedia.org/wiki/Jeff_Hammerbacher\n",
      "https://www.nsf.gov/pubs/2005/nsb0540/\n",
      "https://www.mediawiki.org/\n",
      "https://en.wikipedia.org/wiki/Special:WhatLinksHere/Data_science\n",
      "https://en.wikipedia.org/wiki/Support-vector_machine\n",
      "https://en.wikipedia.org/wiki/Inter-disciplinary\n",
      "https://en.wikipedia.org/wiki/Data_fusion\n",
      "https://api.semanticscholar.org/CorpusID:6107147\n",
      "http://www.worldcat.org/issn/0017-8012\n",
      "https://en.wikipedia.org/wiki/Complex_systems\n",
      "https://en.wikipedia.org/wiki/Communication\n",
      "https://pl.wikipedia.org/wiki/Danologia\n",
      "https://en.wikipedia.org/wiki/Data_compression\n",
      "https://en.wikipedia.org/wiki/Apache_Hadoop\n",
      "https://en.wikipedia.org/wiki/Feature_learning\n",
      "https://en.wikipedia.org/wiki/Data_transformation\n",
      "https://en.wikipedia.org/w/index.php?title=Data_science&action=edit&section=11\n",
      "https://en.wikipedia.org/wiki/Sisense\n",
      "https://en.wikipedia.org/wiki/Special:BookSources/9784431702085\n",
      "https://hbr.org/2012/10/data-scientist-the-sexiest-job-of-the-21st-century\n",
      "https://en.wikipedia.org/wiki/Plotly\n",
      "https://en.wikipedia.org/wiki/Wikipedia:About\n",
      "https://en.wikipedia.org/wiki/Data_security\n",
      "https://pt.wikipedia.org/wiki/Ci%C3%AAncia_de_dados\n",
      "https://www.statisticsviews.com/article/nate-silver-what-i-need-from-statisticians/\n",
      "https://en.wikipedia.org/wiki/Wide-field_Infrared_Survey_Explorer\n",
      "https://en.wikipedia.org/wiki/RapidMiner\n",
      "https://stats.wikimedia.org/#/en.wikipedia.org\n",
      "https://en.wikipedia.org/wiki/David_Donoho\n",
      "https://doi.org/10.1126%2Fscience.1170411\n",
      "https://en.wikipedia.org/wiki/Data_steward\n",
      "https://en.wikipedia.org/wiki/Data_integration\n",
      "http://cacm.acm.org/magazines/2013/12/169933-data-science-and-prediction/fulltext\n",
      "https://flowingdata.com/2009/06/04/rise-of-the-data-scientist/\n",
      "https://en.wikipedia.org/w/index.php?title=Special:UserLogin&returnto=Data+science\n",
      "https://en.wikipedia.org/wiki/Reinforcement_learning\n",
      "https://en.wikipedia.org/wiki/Microsoft_Power_BI\n",
      "https://foundation.wikimedia.org/wiki/Privacy_policy\n",
      "https://en.wikipedia.org/wiki/Peter_Naur\n",
      "https://en.wikipedia.org/wiki/Data_validation\n",
      "https://en.wikipedia.org/wiki/Data_recovery\n",
      "https://en.wikipedia.org/wiki/Data_analysis\n",
      "https://en.wikipedia.org/wiki/Gated_recurrent_unit\n",
      "https://medriscoll.com/post/4740157098/the-three-sexy-skills-of-data-geeks\n",
      "https://www.wikidata.org/wiki/Special:EntityPage/Q2374463\n",
      "https://en.wikipedia.org/wiki/Bootstrap_aggregating\n",
      "https://en.wikipedia.org/wiki/Data_format_management\n",
      "https://en.wikipedia.org/wiki/Supervised_learning\n",
      "https://en.wikipedia.org/wiki/Perceptron\n",
      "https://en.wikipedia.org/wiki/Graphical_model\n",
      "https://api.semanticscholar.org/CorpusID:9743327\n",
      "https://www.stat.purdue.edu/~wsc/\n",
      "http://www.wikimediafoundation.org/\n",
      "https://www.forbes.com/sites/gilpress/2013/08/19/data-science-whats-the-half-life-of-a-buzzword/\n",
      "https://en.wikipedia.org/wiki/Knowledge\n",
      "https://en.wikipedia.org/wiki/U-Net\n",
      "https://en.wikipedia.org/w/index.php?title=Data_science&action=edit&section=1\n",
      "https://sites.engineering.ucsb.edu/~shell/che210d/python.pdf\n",
      "https://en.wikipedia.org/wiki/Data_curation\n",
      "https://en.wikipedia.org/wiki/Cluster_analysis\n",
      "https://en.wikipedia.org/wiki/Data_integrity\n",
      "https://en.wikipedia.org/wiki/Hidden_Markov_model\n",
      "https://ru.wikipedia.org/wiki/%D0%9D%D0%B0%D1%83%D0%BA%D0%B0_%D0%BE_%D0%B4%D0%B0%D0%BD%D0%BD%D1%8B%D1%85\n",
      "https://en.wikipedia.org/wiki/Unstructured_data\n",
      "https://hy.wikipedia.org/wiki/%D5%8F%D5%BE%D5%B5%D5%A1%D5%AC%D5%B6%D5%A5%D6%80%D5%AB_%D5%A3%D5%AB%D5%BF%D5%B8%D6%82%D5%A9%D5%B5%D5%B8%D6%82%D5%B6\n",
      "https://en.wikipedia.org/wiki/Basic_research\n",
      "https://en.wikipedia.org/wiki/Extract,_transform,_load\n",
      "https://en.wikipedia.org/wiki/Data_(computing)\n",
      "https://en.wikipedia.org/w/index.php?title=Data_science&action=edit&section=12\n",
      "https://en.wikipedia.org/wiki/Long_short-term_memory\n",
      "https://en.wikipedia.org/wiki/Empirical_research\n",
      "https://en.wikipedia.org/wiki/Occam_learning\n",
      "https://en.wikipedia.org/w/index.php?title=Data_science&action=edit&section=13\n",
      "https://en.wikipedia.org/wiki/Data_augmentation\n",
      "https://en.wikipedia.org/wiki/Informatics\n",
      "https://en.wikipedia.org/wiki/Data_reduction\n",
      "https://en.wikipedia.org/wiki/Glossary_of_artificial_intelligence\n",
      "https://en.wikipedia.org/wiki/S2CID_(identifier)\n",
      "https://en.wikipedia.org/wiki/Linear_discriminant_analysis\n",
      "https://en.wikipedia.org/wiki/Empirical_risk_minimization\n",
      "https://en.wikipedia.org/wiki/Domain_knowledge\n",
      "https://en.wikipedia.org/wiki/Distributed_computing\n",
      "https://mk.wikipedia.org/wiki/%D0%9D%D0%B0%D1%83%D0%BA%D0%B0_%D0%B7%D0%B0_%D0%BF%D0%BE%D0%B4%D0%B0%D1%82%D0%BE%D1%86%D0%B8\n",
      "https://es.wikipedia.org/wiki/Ciencia_de_datos\n",
      "https://en.wikipedia.org/wiki/Multilayer_perceptron\n",
      "https://en.wikipedia.org/wiki/Regression_analysis\n",
      "https://en.wikipedia.org/wiki/Jupyter_Notebook\n",
      "https://en.wikipedia.org/wiki/Special:RecentChangesLinked/Data_science\n",
      "https://en.wikipedia.org/w/index.php?title=Data_science&action=edit&section=7\n",
      "https://arxiv.org/list/cs.LG/recent\n",
      "https://en.wikipedia.org/wiki/PMID_(identifier)\n",
      "https://en.wikipedia.org/w/index.php?title=Data_science&action=history\n",
      "https://ms.wikipedia.org/wiki/Sains_data\n",
      "https://en.wikipedia.org/wiki/Data_cleansing\n",
      "https://en.wikipedia.org/wiki/Data_mining\n",
      "https://en.wikipedia.org/wiki/Google_Charts\n",
      "https://en.wikipedia.org/wiki/Dataiku\n",
      "https://en.wikipedia.org/wiki/Committee_on_Data_for_Science_and_Technology\n",
      "https://web.archive.org/web/20140102194117/http://simplystatistics.org/2013/12/12/the-key-word-in-data-science-is-not-data-it-is-science/\n",
      "https://en.wikipedia.org/w/index.php?title=Special:CiteThisPage&page=Data_science&id=1012626958&wpFormIdentifier=titleform\n",
      "https://en.wikipedia.org/wiki/Human%E2%80%93computer_interaction\n",
      "https://en.wikipedia.org/wiki/DJ_Patil\n",
      "https://en.wikipedia.org/wiki/Category:Short_description_matches_Wikidata\n",
      "https://en.wikipedia.org/wiki/Independent_component_analysis\n",
      "https://en.wikipedia.org/wiki/Category:Computational_fields_of_study\n",
      "https://en.wikipedia.org/wiki/Wikipedia:Contents\n",
      "https://en.wikipedia.org/wiki/Random_forest\n",
      "https://en.wikipedia.org/wiki/Help:Category\n",
      "https://en.wikipedia.org/wiki/Main_Page\n",
      "https://wikimediafoundation.org/\n",
      "https://en.wikipedia.org/wiki/IBM_Watson_Studio\n",
      "https://en.wikipedia.org/wiki/Anaconda_(Python_distribution)\n",
      "https://en.wikipedia.org/wiki/Statistical_learning_theory\n",
      "https://en.wikipedia.org/wiki/Outline_of_machine_learning\n",
      "https://en.wikipedia.org/w/index.php?title=Template:Data&action=edit\n",
      "https://en.wikipedia.org/wiki/Principal_component_analysis\n",
      "https://en.wikipedia.org/wiki/Data_collection\n",
      "https://en.wikipedia.org/wiki/Conference_on_Neural_Information_Processing_Systems\n",
      "https://en.wikipedia.org/wiki/Support_vector_machine\n",
      "https://en.wikipedia.org/wiki/Python_(programming_language)\n",
      "https://en.wikipedia.org/wiki/Hierarchical_clustering\n",
      "http://foundation.wikimedia.org/wiki/Terms_of_Use\n",
      "https://en.wikipedia.org/wiki/William_S._Cleveland\n",
      "https://en.wikipedia.org/wiki/Statistical_classification\n",
      "https://www.datasciencecentral.com/profiles/blogs/data-science-without-statistics-is-possible-even-desirable\n",
      "https://en.wikipedia.org/w/index.php?title=Data_science&action=info\n",
      "http://www.worldcat.org/oclc/489990740\n",
      "https://en.wikipedia.org/wiki/Statistics\n",
      "https://en.wikipedia.org/wiki/Wikipedia:Manual_of_Style/Stand-alone_lists\n",
      "https://en.wikipedia.org/wiki/Unsupervised_learning\n",
      "https://www.wikidata.org/wiki/Special:EntityPage/Q2374463#sitelinks-wikipedia\n",
      "https://en.wikipedia.org/wiki/Data_farming\n",
      "https://en.wikipedia.org/wiki/Q-learning\n",
      "https://en.wikipedia.org/wiki/Ensemble_learning\n",
      "https://donate.wikimedia.org/wiki/Special:FundraiserRedirector?utm_source=donate&utm_medium=sidebar&utm_campaign=C13_en.wikipedia.org&uselang=en\n",
      "https://en.wikipedia.org/wiki/Restricted_Boltzmann_machine\n",
      "https://cs.wikipedia.org/wiki/Data_science\n",
      "https://en.wikipedia.org/wiki/Wikipedia:LSC\n",
      "https://nl.wikipedia.org/wiki/Datawetenschap\n",
      "https://en.wikipedia.org/wiki/Tableau_Software\n",
      "https://en.wikipedia.org/wiki/Nate_Silver\n",
      "https://en.wikipedia.org/wiki/Journal_of_Machine_Learning_Research\n",
      "https://en.wikipedia.org/wiki/Big_data\n",
      "https://en.wikipedia.org/wiki/Vasant_Dhar\n",
      "https://www.springer.com/book/9784431702085\n",
      "https://en.wikipedia.org/w/index.php?title=Data_science&action=edit&section=10\n",
      "https://en.wikipedia.org/wiki/Data_wrangling\n",
      "https://en.wikipedia.org/wiki/K-nearest_neighbors_classification\n",
      "https://bn.wikipedia.org/wiki/%E0%A6%89%E0%A6%AA%E0%A6%BE%E0%A6%A4%E0%A7%8D%E0%A6%A4_%E0%A6%AC%E0%A6%BF%E0%A6%9C%E0%A7%8D%E0%A6%9E%E0%A6%BE%E0%A6%A8\n",
      "https://en.wikipedia.org/wiki/Computational_learning_theory\n",
      "https://en.wikipedia.org/wiki/ISBN_(identifier)\n",
      "https://en.wikipedia.org/w/index.php?title=Data_science&action=edit\n",
      "https://en.wikipedia.org/wiki/Template:Machine_learning_bar\n",
      "https://en.wikipedia.org/wiki/Conditional_random_field\n",
      "https://en.wikipedia.org/wiki/Template:Data\n",
      "https://en.wikipedia.org/wiki/DBSCAN\n",
      "https://id.wikipedia.org/wiki/Ilmu_data\n",
      "https://zh.wikipedia.org/wiki/%E6%95%B0%E6%8D%AE%E7%A7%91%E5%AD%A6\n",
      "https://en.wikipedia.org/wiki/Learning_to_rank\n",
      "https://en.wikipedia.org/wiki/Non-negative_matrix_factorization\n",
      "https://www.mdpi.com/2504-2289/2/2/14\n",
      "https://en.wikipedia.org/wiki/Information_visualization\n",
      "https://en.wikipedia.org/w/index.php?title=Data_science&action=edit&section=5\n",
      "https://en.wikipedia.org/wiki/File:Multi-Layer_Neural_Network-Vector-Blank.svg\n",
      "https://en.wikipedia.org/wiki/Generative_adversarial_network\n",
      "https://en.wikipedia.org/w/index.php?title=Data_science&action=edit&section=8\n",
      "https://en.wikipedia.org/wiki/Columbia_University\n",
      "https://en.wikipedia.org/wiki/Wikipedia:Community_portal\n",
      "https://en.wikipedia.org/wiki/Association_rule_learning\n",
      "https://en.wikipedia.org/w/index.php?title=Data_science&action=edit&section=6\n",
      "https://en.wikipedia.org/wiki/Data_warehouse\n",
      "https://en.wikipedia.org/wiki/Decision_tree\n",
      "https://en.wikipedia.org/wiki/Bias%E2%80%93variance_tradeoff\n",
      "https://en.wikipedia.org/wiki/Grammar_induction\n",
      "https://de.wikipedia.org/wiki/Data_Science\n",
      "https://en.wikipedia.org/wiki/Space_telescope\n",
      "https://en.wikipedia.org/wiki/Template_talk:Data\n",
      "https://en.wikipedia.org/wiki/Database\n",
      "https://en.wikipedia.org/w/index.php?title=Special:CreateAccount&returnto=Data+science\n",
      "https://en.wikipedia.org/wiki/Naive_Bayes_classifier\n",
      "http://www.worldcat.org/issn/0036-8075\n",
      "https://cran.r-project.org/doc/FAQ/R-FAQ.html#What-is-R_003f\n",
      "https://en.wikipedia.org/w/index.php?title=Data_science&action=edit&section=4\n",
      "https://en.wikipedia.org/wiki/Talk:Data_science\n",
      "https://en.wikipedia.org/wiki/Anomaly_detection\n",
      "https://en.wikipedia.org/wiki/Relevance_vector_machine\n",
      "https://en.wikipedia.org/wiki/Information_science\n",
      "https://en.wikipedia.org/wiki/Webix\n",
      "https://en.wikipedia.org/wiki/Self-organizing_map\n",
      "https://en.wikipedia.org/wiki/Machine_learning\n",
      "https://www.mediawiki.org/wiki/Special:MyLanguage/How_to_contribute\n",
      "https://eu.wikipedia.org/wiki/Datu_zientzia\n",
      "https://www.forbes.com/sites/gilpress/2013/05/28/a-very-short-history-of-data-science/\n",
      "https://en.wikipedia.org/wiki/Category:Information_science\n",
      "https://en.wikipedia.org/wiki/Echo_state_network\n",
      "https://en.wikipedia.org/wiki/Ben_Fry\n",
      "http://en.m.wikipedia.org/w/index.php?title=Data_science&mobileaction=toggle_view_mobile\n",
      "https://en.wikipedia.org/wiki/Special:BookSources/0-12-241770-4\n",
      "https://www.wired.com/2014/07/a-drag-and-drop-toolkit-that-lets-anyone-create-interactive-maps/\n",
      "https://en.wikipedia.org/wiki/Data_management\n",
      "https://en.wikipedia.org/wiki/Cognitive_computing\n",
      "https://en.wikipedia.org/wiki/Local_outlier_factor\n",
      "https://fa.wikipedia.org/wiki/%D8%B9%D9%84%D9%85_%D8%AF%D8%A7%D8%AF%D9%87%E2%80%8C%D9%87%D8%A7\n",
      "https://www.oreilly.com/library/view/doing-data-science/9781449363871/ch01.html\n",
      "https://en.wikipedia.org/wiki/Data_Science\n",
      "https://en.wikipedia.org/wiki/Linear_regression\n",
      "https://en.wikipedia.org/wiki/OCLC_(identifier)\n",
      "https://en.wikipedia.org/wiki/Andrew_Gelman\n",
      "https://en.wikipedia.org/wiki/Probably_approximately_correct_learning\n",
      "https://en.wikipedia.org/wiki/Computation\n",
      "https://en.wikipedia.org/wiki/Comet_NEOWISE\n",
      "https://en.wikipedia.org/wiki/Semi-supervised_learning\n",
      "http://www2.isye.gatech.edu/~jeffwu/presentations/datascience.pdf\n",
      "https://en.wikipedia.org/wiki/Data_pre-processing\n",
      "https://en.wikipedia.org/wiki/Feature_engineering\n",
      "https://th.wikipedia.org/wiki/%E0%B8%A7%E0%B8%B4%E0%B8%97%E0%B8%A2%E0%B8%B2%E0%B8%81%E0%B8%B2%E0%B8%A3%E0%B8%82%E0%B9%89%E0%B8%AD%E0%B8%A1%E0%B8%B9%E0%B8%A5\n",
      "https://en.wikipedia.org/wiki/National_Science_Board\n",
      "https://he.wikipedia.org/wiki/%D7%9E%D7%93%D7%A2_%D7%94%D7%A0%D7%AA%D7%95%D7%A0%D7%99%D7%9D\n",
      "https://en.wikipedia.org/wiki/Structured_prediction\n",
      "https://en.wikipedia.org/wiki/Temporal_difference_learning\n",
      "https://en.wikipedia.org/wiki/Data_visualization\n",
      "https://en.wikipedia.org/wiki/American_Statistical_Association\n",
      "https://ta.wikipedia.org/wiki/%E0%AE%A4%E0%AE%B0%E0%AE%B5%E0%AF%81_%E0%AE%85%E0%AE%B1%E0%AE%BF%E0%AE%B5%E0%AE%BF%E0%AE%AF%E0%AE%B2%E0%AF%8D\n",
      "https://foundation.wikimedia.org/wiki/Cookie_statement\n",
      "https://magazine.amstat.org/blog/2016/06/01/datascience-2/\n",
      "http://priceonomics.com/whats-the-difference-between-data-science-and/\n",
      "https://statmodeling.stat.columbia.edu/2013/11/14/statistics-least-important-part-data-science/\n",
      "https://en.wikipedia.org/wiki/Portal:Current_events\n",
      "https://en.wikipedia.org/wiki/Mathematics\n",
      "https://en.wikipedia.org/wiki/Data_loss\n",
      "http://en.wikipedia.org/wiki/Wikipedia:Contact_us\n",
      "https://benfry.com/phd/dissertation/2.html\n",
      "https://en.wikipedia.org/wiki/Data_migration\n",
      "http://www.datascienceassn.org/about-data-science\n",
      "https://en.wikipedia.org/wiki/R_(programming_language)\n",
      "https://web.archive.org/web/20190620184935/https://magazine.amstat.org/blog/2015/10/01/asa-statement-on-the-role-of-statistics-in-data-science/\n",
      "https://en.wikipedia.org/wiki/File:PIA23792-1600x1200(1).jpg\n",
      "https://en.wikipedia.org/wiki/Decision_tree_learning\n",
      "https://en.wikipedia.org/wiki/Convolutional_neural_network\n",
      "https://en.wikipedia.org/wiki/OPTICS_algorithm\n",
      "https://en.wikipedia.org/wiki/Statistics#Methods\n",
      "https://az.wikipedia.org/wiki/Veril%C9%99nl%C9%99r_elmi_(Data_Science)\n",
      "https://el.wikipedia.org/wiki/%CE%95%CF%80%CE%B9%CF%83%CF%84%CE%AE%CE%BC%CE%B7_%CE%B4%CE%B5%CE%B4%CE%BF%CE%BC%CE%AD%CE%BD%CF%89%CE%BD\n",
      "https://en.wikipedia.org/wiki/Mean-shift\n",
      "https://en.wikipedia.org/wiki/T-distributed_stochastic_neighbor_embedding\n",
      "http://simplystatistics.org/2013/12/12/the-key-word-in-data-science-is-not-data-it-is-science/\n",
      "https://en.wikipedia.org/wiki/Category:Data_analysis\n",
      "https://en.wikipedia.org/wiki/Automated_machine_learning\n",
      "https://en.wikipedia.org/wiki/Dimensionality_reduction\n",
      "https://en.wikipedia.org/wiki/Qlik\n",
      "https://en.wikipedia.org/wiki/Template_talk:Machine_learning_bar\n",
      "https://fi.wikipedia.org/wiki/Datatiede\n",
      "https://en.wikipedia.org/wiki/Online_machine_learning\n",
      "https://magazine.amstat.org/blog/2015/10/01/asa-statement-on-the-role-of-statistics-in-data-science/\n",
      "https://vi.wikipedia.org/wiki/Khoa_h%E1%BB%8Dc_d%E1%BB%AF_li%E1%BB%87u\n",
      "https://en.wikipedia.org/wiki/Montpellier_2_University\n",
      "https://en.wikipedia.org/wiki/Proper_generalized_decomposition\n",
      "https://en.wikipedia.org/wiki/Special:BookSources/978-0-9825442-0-4\n",
      "https://www2.isye.gatech.edu/~jeffwu/publications/fazhan.pdf\n",
      "https://en.wikipedia.org/wiki/DeepDream\n",
      "https://kk.wikipedia.org/wiki/%D0%94%D0%B5%D1%80%D0%B5%D0%BA%D1%82%D0%B5%D1%80_%D1%82%D1%83%D1%80%D0%B0%D0%BB%D1%8B_%D2%93%D1%8B%D0%BB%D1%8B%D0%BC\n",
      "https://en.wikipedia.org/wiki/Data_scraping\n",
      "https://en.wikipedia.org/w/index.php?title=Data_science&action=edit&section=9\n",
      "https://fr.wikipedia.org/wiki/Science_des_donn%C3%A9es\n",
      "https://en.wikipedia.org/wiki/Category:Lists_having_no_precise_inclusion_criteria_from_June_2020\n",
      "https://en.wikipedia.org/wiki/Special:Random\n",
      "https://www.forbes.com/sites/peterpham/2015/08/28/the-impacts-of-big-data-that-you-may-not-have-heard-of/\n",
      "https://en.wikipedia.org/wiki/Transformer_(machine_learning_model)\n",
      "http://en.wikipedia.org/wiki/Wikipedia:Text_of_Creative_Commons_Attribution-ShareAlike_3.0_Unported_License\n",
      "https://en.wikipedia.org/wiki/Memtransistor\n",
      "https://en.wikipedia.org/wiki/Canonical_correlation\n",
      "http://courses.csail.mit.edu/18.337/2015/docs/50YearsDataScience.pdf\n",
      "https://en.wikipedia.org/wiki/Data_editing\n",
      "https://en.wikipedia.org/wiki/Data_preservation\n",
      "https://en.wikipedia.org/w/index.php?title=Data_science&action=edit&section=3\n",
      "https://en.wikipedia.org/wiki/Information_technology\n",
      "https://ur.wikipedia.org/wiki/%DA%88%DB%8C%D9%B9%D8%A7_%D8%B3%D8%A7%D8%A6%D9%86%D8%B3\n",
      "https://en.wikipedia.org/wiki/Bayesian_network\n",
      "http://pubmed.ncbi.nlm.nih.gov/19265007\n",
      "https://uk.wikipedia.org/wiki/%D0%9D%D0%B0%D1%83%D0%BA%D0%B0_%D0%BF%D1%80%D0%BE_%D0%B4%D0%B0%D0%BD%D1%96\n",
      "https://en.wikipedia.org/wiki/Help:Contents\n",
      "https://en.wikipedia.org/wiki/John_Tukey\n",
      "https://en.wikipedia.org/wiki/Electrochemical_RAM\n",
      "https://en.wikipedia.org/wiki/Machine_Learning_(journal)\n",
      "https://en.wikipedia.org/wiki/Special:SpecialPages\n",
      "https://en.wikipedia.org/wiki/CURE_data_clustering_algorithm\n",
      "https://en.wikipedia.org/wiki/Jeffrey_T._Leek\n",
      "https://doi.org/10.3390%2Fbdcc2020014\n",
      "https://en.wikipedia.org/wiki/Category:Use_dmy_dates_from_December_2012\n",
      "https://ja.wikipedia.org/wiki/%E3%83%87%E3%83%BC%E3%82%BF%E3%82%B5%E3%82%A4%E3%82%A8%E3%83%B3%E3%82%B9\n",
      "https://en.wikipedia.org/wiki/PolyAnalyst\n",
      "https://en.wikipedia.org/wiki/Computational_science\n",
      "https://towardsdatascience.com/how-data-science-will-impact-future-of-businesses-7f11f5699c4d\n",
      "https://en.wikipedia.org/wiki/Category:All_lists_having_no_precise_inclusion_criteria\n",
      "http://foundation.wikimedia.org/wiki/Privacy_policy\n",
      "https://hi.wikipedia.org/wiki/%E0%A4%86%E0%A4%81%E0%A4%95%E0%A4%A1%E0%A4%BC%E0%A4%BE_%E0%A4%B5%E0%A4%BF%E0%A4%9C%E0%A5%8D%E0%A4%9E%E0%A4%BE%E0%A4%A8\n",
      "https://en.wikipedia.org/wiki/MATLAB\n",
      "https://en.wikipedia.org/wiki/Turing_award\n",
      "https://zh-yue.wikipedia.org/wiki/%E6%95%B8%E6%93%9A%E7%A7%91%E5%AD%B8\n",
      "https://ar.wikipedia.org/wiki/%D8%B9%D9%84%D9%85_%D8%A7%D9%84%D8%A8%D9%8A%D8%A7%D9%86%D8%A7%D8%AA\n",
      "https://en.wikipedia.org/wiki/Data_degradation\n",
      "https://en.wikipedia.org/wiki/Category:Computer_occupations\n",
      "https://en.wikipedia.org/wiki/Jim_Gray_(computer_scientist)\n",
      "https://en.wikipedia.org/wiki/Data_quality\n",
      "https://en.wikipedia.org/wiki/Nathan_Yau\n",
      "https://en.wikipedia.org/wiki/Vapnik%E2%80%93Chervonenkis_theory\n",
      "https://en.wikipedia.org/wiki/Computer_science\n",
      "https://en.wikipedia.org/wiki/Factor_analysis\n",
      "https://en.wikipedia.org/wiki/Data_scrubbing\n",
      "https://doi.org/10.1145%2F2500499\n",
      "https://en.wikipedia.org/wiki/Category:CS1_maint:_others\n",
      "https://en.wikipedia.org/wiki/Data_corruption\n",
      "https://my.wikipedia.org/wiki/%E1%80%A1%E1%80%81%E1%80%BB%E1%80%80%E1%80%BA%E1%80%A1%E1%80%9C%E1%80%80%E1%80%BA%E1%80%9E%E1%80%AD%E1%80%95%E1%80%B9%E1%80%95%E1%80%B6%E1%80%95%E1%80%8A%E1%80%AC\n",
      "https://en.wikipedia.org/wiki/Data_storage\n",
      "https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm\n",
      "https://en.wikipedia.org/wiki/Doi_(identifier)\n",
      "https://en.wikipedia.org/wiki/Boosting_(machine_learning)\n",
      "https://en.wikipedia.org/w/index.php?title=Data_science&oldid=1012626958\n",
      "https://en.wikipedia.org/w/index.php?title=Data_science&action=edit&section=2\n",
      "https://en.wikipedia.org/wiki/Artificial_neural_network\n",
      "https://en.wikipedia.org/wiki/Astronomical_survey\n",
      "https://en.wikipedia.org/wiki/Special:RecentChanges\n",
      "https://en.wikipedia.org/wiki/TensorFlow\n",
      "https://web.archive.org/web/20170320193019/https://books.google.com/books?id=oGs_AQAAIAAJ\n",
      "https://en.wikipedia.org/wiki/Deep_learning\n",
      "https://en.wikipedia.org/wiki/Spiking_neural_network\n"
     ]
    }
   ],
   "source": [
    "### YOUR CODE HERE duplicate?\n",
    "urls = open('./urls.txt', 'r')\n",
    "\n",
    "url = urls.readlines()\n",
    "url = list(map(lambda s: s.strip(), url))\n",
    "\n",
    "url_list = []\n",
    "\n",
    "response = requests.get(url[0])\n",
    "if response.status_code==200:\n",
    "    html = response.text\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    paragraphs = soup.find_all(\"a\")\n",
    "    \n",
    "    links = []\n",
    "    http = 'https://en.wikipedia.org'\n",
    "    for link in paragraphs:\n",
    "        web_link = link.get('href')\n",
    "        #string = str(web_link)\n",
    "            \n",
    "\n",
    "        string = str(web_link)\n",
    "        #print(string)\n",
    "        if string == None:\n",
    "            continue\n",
    "        if string.startswith('http'):\n",
    "            url_list.append(string)\n",
    "        elif string.startswith('//'):\n",
    "            temp = 'http:'+string\n",
    "            url_list.append(temp)\n",
    "        elif string.startswith('#'):\n",
    "            continue\n",
    "        elif string.startswith('/wiki'):\n",
    "            temp = http + string\n",
    "            url_list.append(temp)\n",
    "        elif string.startswith('/w/'):\n",
    "            temp = http + string\n",
    "            url_list.append(temp)\n",
    "            \n",
    "            \n",
    "               \n",
    "else:\n",
    "    print(response.status_code)\n",
    "\n",
    "url_list = list(set(url_list))\n",
    "\n",
    "    \n",
    "urls.close()\n",
    "file2 = open('./Q2.txt', 'w')\n",
    "length = len(url_list)\n",
    "file2.write(str(length) + '\\n')\n",
    "print(str(length))\n",
    "for link in url_list:\n",
    "    file2.write(link + '\\n')\n",
    "    print(link)\n",
    "   \n",
    "\"\"\"for i in range(0, length):\n",
    "    file2.write(url_list[i] + '\\n')\n",
    "\"\"\"    \n",
    "file2.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "# Q3: Part 1 (10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "1. Retrieve and parse multiple web pages. The text file \"urls.txt\" contains a list of webpages to be parsed. Each line in the text file corresponds to a url. Use BeautifulSoup to fetch each webpage and parse it as specified below. \n",
    "\n",
    "2. For each webpage document do the following:\n",
    "    1. Retrieve all text enclosed in paragraph tags. \n",
    "    2. Convert the text to lowercase. \n",
    "    3. Strip out punctuation. Note: if you use translate() with string.punctuation, then it may not strip out all characters. Use a regular expression involving \\W to strip out all non alpha-numeric characters.\n",
    "    4. Tokenize into words based on whitespace separation.\n",
    "\n",
    "3. Find the number of unique words in each webpage document. \n",
    "\n",
    "4. Find the Length of each webpage document. The length of a document is defined as the total number of words in the document (not just unique words).\n",
    "\n",
    "5. For each of the following words: “statistics”, “analytics”, “data”, and “science”, \n",
    "    a. Find Term Frequency (tf). \n",
    "    The term frequency (tf) of a term (word) is defined as the number of times that term t occurs in document d, \n",
    "    divided by the total number of words in the document. \n",
    "    The tf of a word depends on the document under consideration. \n",
    "    \n",
    "    b. Find Inverse Document Frequency (idf).\n",
    "    The inverse document frequency of a word is the logarithmically scaled inverse fraction of the documents that \n",
    "    contain the word, obtained by dividing the total number of documents by the number of documents containing the \n",
    "    term, and then taking the logarithm of that ratio.\n",
    "    The idf of a word doesn't depend on any documnet in which the word is present. \n",
    "    To calculate the idf, you will have to use the log function. The base for the log function must be e.\n",
    "    \n",
    "    c. Find tf-idf. \n",
    "    The tf-idf of a word is the product of the term frequency of the word in document d, and its inverse document \n",
    "    frequency. \n",
    "    The tf-idf of a word depends on the document under consideration. \n",
    "    \n",
    "    Reference: https://en.wikipedia.org/wiki/Tf%E2%80%93idf\n",
    "    \n",
    "The output should be written to an output file named \"Q3_Part1.txt\".\n",
    "\n",
    "The format of the output file is as shown below:\n",
    "\n",
    "1. Number of unique words in documents: [702, 723, 280]\n",
    "\n",
    "2. Length of documents: [1711, 1928, 563]\n",
    "\n",
    "3. tf\n",
    "    statistics: [0.0070134424313267095, 0.0025933609958506223, 0.0]\n",
    "    analytics: [0.0029222676797194622, 0.0031120331950207467, 0.0]\n",
    "    data: [0.056107539450613676, 0.05446058091286307, 0.0]\n",
    "    science: [0.03798947983635301, 0.011410788381742738, 0.028419182948490232]\n",
    "    \n",
    "4. idf \n",
    "    statistics: 0.510825623766\n",
    "    analytics: 0.510825623766\n",
    "    data: 0.223143551314\n",
    "    science: 0.0\n",
    "    \n",
    "5. tf-idf\n",
    "    statistics: [0.0028437061936282715, 0.0010515173965460695, 0.0]\n",
    "    analytics: [0.0011848775806784465, 0.0012618208758552834, 0.0]\n",
    "    data: [0.022749649549026172, 0.022081865327467459, 0.0]\n",
    "    science: [0.0, 0.0, 0.0]\n",
    "   \n",
    "The above values are for the first three webpage urls given in the file \"urls.txt\". The number of unique words in documents, average length of documents, tf and tf-idf values for the four words, must be in the order of the urls given in \"urls.txt\".\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Number of unique words in documents:['473', '1975', '727', '913', '1170']\n",
      "\n",
      "\n",
      "2. Length of documents: ['1049', '6306', '1950', '2339', '3769']\n",
      "\n",
      "\n",
      "3. tf \n",
      "statistics: [0.017159199237368923, 0.0003171582619727244, 0.002564102564102564, 0.0, 0.003183868400106129]\n",
      "analytics: [0.0009532888465204957, 0.0028544243577545195, 0.003076923076923077, 0.00042753313381787086, 0.0010612894667020429]\n",
      "data: [0.06482364156339371, 0.04551221059308595, 0.055384615384615386, 0.0, 0.04775802600159194]\n",
      "science: [0.03813155386081983, 0.0017443704408499842, 0.011282051282051283, 0.0017101325352714834, 0.016184664367206156]\n",
      "\n",
      "\n",
      "4. idf \n",
      "statistics: 0.22314355131420976\n",
      "analytics: 0.0\n",
      "data: 0.22314355131420976\n",
      "science: 0.0\n",
      "\n",
      "\n",
      "5. tf-idf\n",
      "statistics: [0.0038289646555345813, 7.07718209052362e-05, 0.0005721629520877173, 0.0, 0.000710459701716773]\n",
      "analytics: [0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "data: [0.014464977587575084, 0.010155756299901397, 0.012358719765094695, 0.0, 0.010656895525751595]\n",
      "science: [0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### YOUR CODE HERE\n",
    "import string\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "urls = open('./urls.txt', 'r', encoding='utf-8')\n",
    "\n",
    "url = urls.readlines()\n",
    "url = list(map(lambda s: s.strip(), url))\n",
    "\n",
    "url_list = []\n",
    "\n",
    "d = {'str':['none'], 'count':[0]}\n",
    "df = pd.DataFrame(data=d)\n",
    "\n",
    "unique = []\n",
    "word_len = []\n",
    "find_word = [\"statistics\", \"analytics\", \"data\", \"science\"]\n",
    "sta = []\n",
    "ana = []\n",
    "data = []\n",
    "sci = []\n",
    "\n",
    "sta_2 = 0\n",
    "ana_2 = 0\n",
    "data_2 = 0\n",
    "sci_2 = 0\n",
    "\n",
    "\n",
    "for i in range(0, 5):\n",
    "    response = requests.get(url[i])\n",
    "    if response.status_code==200:\n",
    "        html = response.text\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "        paragraphs = soup.find_all(\"p\")\n",
    "        \n",
    "        text = \"\"\n",
    "        #merge all text\n",
    "        for p in paragraphs:\n",
    "            p_text = p.get_text()\n",
    "            #p_text = p_text.lower()\n",
    "            if text==\"\":\n",
    "                text = p_text\n",
    "            else: \n",
    "                text = text + \" \" + p_text\n",
    "        #lower\n",
    "        text = text.lower()\n",
    "        #print(text)\n",
    "        \n",
    "        #punctuation and tokenize\n",
    "        text_p = re.sub(r'[\\W]',' ',text)\n",
    "        text_list = text_p.split(\" \")\n",
    "        \n",
    "\n",
    "        \n",
    "        \n",
    "        text_list = list(filter(None, text_list))\n",
    "        \"\"\"for te in text_list:\n",
    "            print(te)\"\"\"\n",
    "        #\n",
    "       \n",
    "        \n",
    "        \n",
    "        #2\n",
    "        word_len.append(str(len(text_list)))\n",
    "        #remove duplication\n",
    "        text_dupl = list(set(text_list))\n",
    "        #1\n",
    "        unique.append(str(len(text_dupl)))\n",
    "        \n",
    "        #3\n",
    "        sta.append(text_list.count(find_word[0])/ float(word_len[i]))\n",
    "        ana.append(text_list.count(find_word[1])/ float(word_len[i]))\n",
    "        data.append(text_list.count(find_word[2])/ float(word_len[i]))\n",
    "        sci.append(text_list.count(find_word[3])/ float(word_len[i]))\n",
    "        #4\n",
    "        if find_word[0] in text_dupl:\n",
    "            sta_2 += 1\n",
    "        if find_word[1] in text_dupl:\n",
    "            ana_2 += 1\n",
    "        if find_word[2] in text_dupl:\n",
    "            data_2 += 1\n",
    "        if find_word[3] in text_dupl:\n",
    "            sci_2 += 1\n",
    "        \n",
    "            \n",
    "            \n",
    "            \n",
    "    else:\n",
    "        print(response.status_code)\n",
    "\n",
    "      \n",
    "        \n",
    "        \n",
    "urls.close()\n",
    "\n",
    "#1\n",
    "\n",
    "sta_2 = math.log(1 / (sta_2 / 5))\n",
    "ana_2 = math.log(1 / (ana_2 / 5))\n",
    "data_2 = math.log(1 / (data_2 / 5))\n",
    "sci_2 = math.log(1/ (sci_2 / 5))\n",
    "idf = []\n",
    "idf.append(sta_2)\n",
    "idf.append(ana_2)\n",
    "idf.append(data_2)\n",
    "idf.append(sci_2)\n",
    "#5\n",
    "sta_3 = []\n",
    "ana_3 = []\n",
    "data_3 = []\n",
    "sci_3 = []\n",
    "for i in range(0, 5):\n",
    "    sta_3.append(sta[i] * sta_2)\n",
    "    ana_3.append(ana[i] * ana_2)\n",
    "    data_3.append(data[i] * data_2)\n",
    "    sci_3.append(sci[i] * sci_2)\n",
    "\n",
    "Q3 = open('./Q3_Part1.txt', 'w', encoding='utf-8')\n",
    "Q3.write(\"1. Number of unique words in documents:%s\\n\\n\" % str(unique))\n",
    "print(\"1. Number of unique words in documents:%s\\n\\n\" % str(unique))\n",
    "Q3.write(\"2. Length of documents: %s\\n\\n\" % str(word_len))\n",
    "print(\"2. Length of documents: %s\\n\\n\" % str(word_len))\n",
    "Q3.write(\"3. tf \\nstatistics: %s\\nanalytics: %s\\ndata: %s\\nscience: %s\\n\\n\" % (str(sta), str(ana), str(data), str(sci)))\n",
    "print(\"3. tf \\nstatistics: %s\\nanalytics: %s\\ndata: %s\\nscience: %s\\n\\n\" % (str(sta), str(ana), str(data), str(sci)))\n",
    "Q3.write(\"4. idf \\nstatistics: %s\\nanalytics: %s\\ndata: %s\\nscience: %s\\n\\n\" %(str(sta_2), str(ana_2), str(data_2), str(sci_2)))\n",
    "print(\"4. idf \\nstatistics: %s\\nanalytics: %s\\ndata: %s\\nscience: %s\\n\\n\" %(str(sta_2), str(ana_2), str(data_2), str(sci_2)))\n",
    "Q3.write(\"5. tf-idf\\nstatistics: %s\\nanalytics: %s\\ndata: %s\\nscience: %s\\n\\n\" %(str(sta_3), str(ana_3), str(data_3), str(sci_3)))\n",
    "print(\"5. tf-idf\\nstatistics: %s\\nanalytics: %s\\ndata: %s\\nscience: %s\\n\\n\" %(str(sta_3), str(ana_3), str(data_3), str(sci_3)))\n",
    "Q3.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "# Q3: Part 2 (10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "Repeat Part 1, but first remove the stop words given in the file \"stop_words.txt\".  \n",
    "The ouput for Part 2 should have the same format as Part 1, and should be written to an output file named \"Q3_Part2.txt\".\n",
    "Note: The length of document, in this case, will not include stop words. Similarly, the number of unique words in documents, and the calculation of tf, idf, tf-idf should be done after removing the stop words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "Sample output for the file \"urls.txt\"\n",
    "\n",
    "1. Number of unique words in documents: [596, 600, 231]\n",
    "\n",
    "2. Length of documents: [1020, 1079, 357]  \n",
    "\n",
    "3. tf\n",
    "    statistics: [0.011764705882352941, 0.004633920296570899, 0.0]\n",
    "    analytics: [0.004901960784313725, 0.005560704355885079, 0.0]\n",
    "    data: [0.09411764705882353, 0.09731232622798888, 0.0]\n",
    "    science: [0.06372549019607843, 0.020389249304911955, 0.04481792717086835]\n",
    "    \n",
    "4. idf \n",
    "    statistics: 0.405465108108\n",
    "    analytics: 0.405465108108\n",
    "    data: 0.405465108108\n",
    "    science: 0.0\n",
    "    \n",
    "5. tf-idf\n",
    "    statistics: [0.0047701777424489925, 0.0018788929940137366, 0.0]\n",
    "    analytics: [0.0019875740593537469, 0.002254671592816484, 0.0]\n",
    "    data: [0.03816142193959194, 0.039456752874288473, 0.0]\n",
    "    science: [0.0, 0.0, 0.0] \n",
    "      \n",
    "The above values are for the first three webpage urls given in the file \"urls.txt\". The number of unique words in documents, average length of documents, tf and tf-idf values for the four words, must be in the order of the urls given in \"urls.txt\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Number of unique words in documents:['389', '1794', '606', '809', '1017']\n",
      "\n",
      "\n",
      "2. Length of documents: ['644', '3837', '1102', '1481', '2109']\n",
      "\n",
      "\n",
      "3. tf \n",
      "statistics: [0.027950310559006212, 0.0005212405525149857, 0.004537205081669692, 0.0, 0.005689900426742532]\n",
      "analytics: [0.0015527950310559005, 0.004691164972634871, 0.0054446460980036296, 0.0006752194463200541, 0.001896633475580844]\n",
      "data: [0.10559006211180125, 0.07479801928590045, 0.09800362976406533, 0.0, 0.08534850640113797]\n",
      "science: [0.062111801242236024, 0.0028668230388324213, 0.019963702359346643, 0.0027008777852802163, 0.028923660502607872]\n",
      "\n",
      "\n",
      "4. idf \n",
      "statistics: 0.22314355131420976\n",
      "analytics: 0.0\n",
      "data: 0.22314355131420976\n",
      "science: 0.0\n",
      "\n",
      "\n",
      "5. tf-idf\n",
      "statistics: [0.006236931558471701, 0.00011631146797717476, 0.0010124480549646541, 0.0, 0.0012696645878475662]\n",
      "analytics: [0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "data: [0.02356174144311532, 0.01669069565472458, 0.021868877987236527, 0.0, 0.019044968817713493]\n",
      "science: [0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### YOUR CODE HERE\n",
    "### YOUR CODE HERE\n",
    "urls = open('./urls.txt', 'r', encoding='utf-8')\n",
    "url = urls.readlines()\n",
    "url = list(map(lambda s: s.strip(), url))\n",
    "stop_words = pd.read_csv('./stop_words.txt', engine='python', encoding='UTF-8', sep='\\n', header=None)\n",
    "\n",
    "url_list = []\n",
    "\n",
    "unique = []\n",
    "word_len = []\n",
    "find_word = [\"statistics\", \"analytics\", \"data\", \"science\"]\n",
    "sta = []\n",
    "ana = []\n",
    "data = []\n",
    "sci = []\n",
    "\n",
    "sta_2 = 0\n",
    "ana_2 = 0\n",
    "data_2 = 0\n",
    "sci_2 = 0\n",
    "\n",
    "\n",
    "for i in range(0, 5):\n",
    "    response = requests.get(url[i])\n",
    "    if response.status_code==200:\n",
    "        html = response.text\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "        paragraphs = soup.find_all(\"p\")\n",
    "        \n",
    "        text = \"\"\n",
    "        #merge all text\n",
    "        for p in paragraphs:\n",
    "            p_text = p.get_text()\n",
    "            #p_text = p_text.lower()\n",
    "            if text==\"\":\n",
    "                text = p_text\n",
    "            else: \n",
    "                text = text + \" \" + p_text\n",
    "        #lower\n",
    "        text = text.lower()\n",
    "        \n",
    "        #punctuation and tokenize\n",
    "        text_p = re.sub(r'[\\W]',' ',text)\n",
    "        text_list = text_p.split(\" \")\n",
    "        text_list = list(filter(None, text_list))\n",
    "        \n",
    "        \n",
    "        for index, word in stop_words.iterrows():\n",
    "            while word.iloc[0] in text_list:\n",
    "                text_list.remove(word.iloc[0])\n",
    "    \n",
    "        \n",
    "        \n",
    "        #remove duplication\n",
    "        text_dupl = list(set(text_list))\n",
    "        #1\n",
    "        unique.append(str(len(text_dupl)))\n",
    "        #2\n",
    "        word_len.append(str(len(text_list)))\n",
    "        #3\n",
    "        sta.append(text_list.count(find_word[0])/ int(word_len[i]))\n",
    "        ana.append(text_list.count(find_word[1])/ int(word_len[i]))\n",
    "        data.append(text_list.count(find_word[2])/ int(word_len[i]))\n",
    "        sci.append(text_list.count(find_word[3])/ int(word_len[i]))\n",
    "        #4\n",
    "        if find_word[0] in text_dupl:\n",
    "            sta_2 += 1\n",
    "        if find_word[1] in text_dupl:\n",
    "            ana_2 += 1\n",
    "        if find_word[2] in text_dupl:\n",
    "            data_2 += 1\n",
    "        if find_word[3] in text_dupl:\n",
    "            sci_2 += 1\n",
    "    \n",
    "        \n",
    "        \n",
    "            \n",
    "            \n",
    "            \n",
    "    else:\n",
    "        print(response.status_code)\n",
    "\n",
    "      \n",
    "        \n",
    "        \n",
    "urls.close()\n",
    "#1\n",
    "#2\n",
    "#3\n",
    "#4\n",
    "sta_2 = math.log(1 / (sta_2 / 5))\n",
    "ana_2 = math.log(1 / (ana_2 / 5))\n",
    "data_2 = math.log(1 / (data_2 / 5))\n",
    "sci_2 = math.log(1/ (sci_2 / 5))\n",
    "idf = []\n",
    "idf.append(sta_2)\n",
    "idf.append(ana_2)\n",
    "idf.append(data_2)\n",
    "idf.append(sci_2)\n",
    "#5\n",
    "sta_3 = []\n",
    "ana_3 = []\n",
    "data_3 = []\n",
    "sci_3 = []\n",
    "for i in range(0, 5):\n",
    "    sta_3.append(sta[i] * sta_2)\n",
    "    ana_3.append(ana[i] * ana_2)\n",
    "    data_3.append(data[i] * data_2)\n",
    "    sci_3.append(sci[i] * sci_2)\n",
    "\n",
    "Q3 = open('./Q3_Part2.txt', 'w', encoding='utf-8')\n",
    "Q3.write(\"1. Number of unique words in documents:%s\\n\\n\" % str(unique))\n",
    "print(\"1. Number of unique words in documents:%s\\n\\n\" % str(unique))\n",
    "Q3.write(\"2. Length of documents: %s\\n\\n\" % str(word_len))\n",
    "print(\"2. Length of documents: %s\\n\\n\" % str(word_len))\n",
    "Q3.write(\"3. tf \\nstatistics: %s\\nanalytics: %s\\ndata: %s\\nscience: %s\\n\\n\" % (str(sta), str(ana), str(data), str(sci)))\n",
    "print(\"3. tf \\nstatistics: %s\\nanalytics: %s\\ndata: %s\\nscience: %s\\n\\n\" % (str(sta), str(ana), str(data), str(sci)))\n",
    "Q3.write(\"4. idf \\nstatistics: %s\\nanalytics: %s\\ndata: %s\\nscience: %s\\n\\n\" %(str(sta_2), str(ana_2), str(data_2), str(sci_2)))\n",
    "print(\"4. idf \\nstatistics: %s\\nanalytics: %s\\ndata: %s\\nscience: %s\\n\\n\" %(str(sta_2), str(ana_2), str(data_2), str(sci_2)))\n",
    "Q3.write(\"5. tf-idf\\nstatistics: %s\\nanalytics: %s\\ndata: %s\\nscience: %s\\n\\n\" %(str(sta_3), str(ana_3), str(data_3), str(sci_3)))\n",
    "print(\"5. tf-idf\\nstatistics: %s\\nanalytics: %s\\ndata: %s\\nscience: %s\\n\\n\" %(str(sta_3), str(ana_3), str(data_3), str(sci_3)))\n",
    "Q3.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
